{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "import os\n",
    "import numpy as np\n",
    "from custom_dataset import CustomDataset, ResidualBlock, ResNet,CustomResNet50, GCN\n",
    "\n",
    "import plotly.graph_objects as go\n",
    "from torch_geometric.nn import MLP\n",
    "from pytorch_metric_learning.losses import NTXentLoss\n",
    "from torch.utils.data import random_split\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "MUSIC_DIRECTORY = 'Music'# Sets the music directory variable to 'MUSIC' which is a folder that stores spectograms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize an empty list to store the paths of PNG files\n",
    "file_paths = []\n",
    "\n",
    "# Initialize an empty list to keep track of already processed files\n",
    "processed_files = []\n",
    "\n",
    "# Walk through the 'Music' directory\n",
    "for directory, _, files in os.walk('Music'):\n",
    "    # Filter out files that end with '.png' and have not been processed yet\n",
    "    png_files = [f for f in files if f.endswith('.png') and f not in processed_files]\n",
    "    \n",
    "    # If there are any new PNG files found\n",
    "    if png_files:\n",
    "        # Add the full paths of the new PNG files to the file_paths list\n",
    "        file_paths += [os.path.join(directory, f) for f in png_files]\n",
    "        \n",
    "        # Add the new PNG files to the processed_files list to avoid duplicates\n",
    "        processed_files += png_files\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an instance of the CustomDataset class with the music_directory set to MUSIC_DIRECTORY\n",
    "custom_dataset = CustomDataset(music_directory=MUSIC_DIRECTORY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "300"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(custom_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataLoader instance for the custom_dataset\n",
    "# Set the batch size to 30 and enable shuffling of the data\n",
    "train_loader = DataLoader(custom_dataset, batch_size=30, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine the device to use for PyTorch operations\n",
    "# Use CUDA if it is available, otherwise fallback to CPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an instance of the ResNet model with ResidualBlock and specified layers\n",
    "# Move the model to the specified device (either CUDA or CPU)\n",
    "model = ResNet(ResidualBlock, [3, 4, 6, 3]).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the loss function with a specified temperature parameter\n",
    "loss_func = NTXentLoss(temperature=0.1)\n",
    "\n",
    "# Initialize the Adam optimizer with the parameters of the model\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "\n",
    "# Initialize the learning rate scheduler with the optimizer, \n",
    "# set the step size to 10 and the gamma (decay factor) to 0.5\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer=optimizer, step_size=10, gamma=0.5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the training function\n",
    "def train():\n",
    "    model.train()  # Set the model to training mode\n",
    "    total_loss = 0  # Initialize the total loss\n",
    "    total_pairs = 0  # Initialize to keep track of the total number of pairs processed\n",
    "    outputs_cnn_list = []\n",
    "\n",
    "    # Iterate through the DataLoader\n",
    "    for batch_idx, (patch1, patch2, index, filename) in enumerate(tqdm(train_loader)):\n",
    "        patch1 = patch1.to(device)  # Move the first patch to the specified device\n",
    "        patch2 = patch2.to(device)  # Move the second patch to the specified device\n",
    "\n",
    "        optimizer.zero_grad()  # Zero the gradients\n",
    "\n",
    "        # Forward pass through the model\n",
    "        #Output1 is the outpout after it has gone through the mlp and output1_cnn is the 512 channel cnn output \n",
    "        output_1, output_2, output1_cnn, output2_cnn = model(patch1, patch2)\n",
    "\n",
    "        # Prepare embeddings for loss computation\n",
    "        embeddings = torch.cat((output_1, output_2))  # Concatenate the output embeddings\n",
    "        batch_size = patch1.size(0)  # Get the batch size\n",
    "        indices = torch.arange(0, batch_size, device=device)  # Create a tensor of indices\n",
    "        labels = torch.cat((indices, indices))  # Create the labels for NTXentLoss\n",
    "        loss = loss_func(embeddings, labels)  # Compute the loss\n",
    "\n",
    "        loss.backward()  # Backpropagate the loss\n",
    "        optimizer.step()  # Update the model parameters\n",
    "\n",
    "        total_loss += loss.item() * batch_size  # Accumulate the loss\n",
    "        total_pairs += batch_size  # Update the total number of pairs processed\n",
    "\n",
    "        # Convert output1_cnn to numpy and store in list\n",
    "        outputs_cnn_np = output1_cnn.detach().cpu().numpy()\n",
    "        outputs_cnn_list.append(outputs_cnn_np)\n",
    "\n",
    "    return total_loss / total_pairs, outputs_cnn_list  # Return the average loss per pair\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize an empty list to store loss values for each epoch\n",
    "loss_values = []\n",
    "\n",
    "# Iterate through 200 epochs\n",
    "for epoch in tqdm(range(200)):\n",
    "    # Call the train function and get the loss for the current epoch\n",
    "    loss,outputs_cnn_list = train()\n",
    "    \n",
    "    # Print the epoch number and the corresponding loss value\n",
    "    print(f\"Epoch {epoch:03d}, Loss {loss:.4f}\")\n",
    "    \n",
    "    # Append the loss value to the loss_values list\n",
    "    loss_values.append(loss)\n",
    "    \n",
    "    # Step the learning rate scheduler\n",
    "    scheduler.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate the list of CNN outputs along the first axis\n",
    "outputs_cnn_array = np.concatenate(outputs_cnn_list, axis=0)\n",
    "\n",
    "# Save the concatenated CNN outputs array to a numpy file\n",
    "np.save(\"outputs_cnn.npy\", outputs_cnn_array)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_values=np.load(\"outputs_cnn.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = ResNet(ResidualBlock, [3, 4, 6, 3]).to(device)\n",
    "\n",
    "# Load the model checkpoint from the specified file\n",
    "checkpoint = torch.load(\"model.pth\")\n",
    "\n",
    "# Load the state dictionary from the checkpoint into the model\n",
    "model.load_state_dict(checkpoint)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()  # Set the model to evaluation mode\n",
    "all_outputs = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad():  # Disable gradient computation\n",
    "    for input1, input2, labels_index, labels_name  in train_loader:\n",
    "        input1 = input1.to(device)\n",
    "        input2 = input2.to(device)\n",
    "        _, _, output1_cnn, _ = model(input1, input2)\n",
    "        output1_cnn = output1_cnn.cpu().detach()\n",
    "\n",
    "        all_outputs.append(output1_cnn)\n",
    "        all_labels.append(labels_index.cpu())\n",
    "\n",
    "# Concatenate all collected outputs and labels\n",
    "all_outputs = torch.cat(all_outputs).squeeze()\n",
    "all_labels = torch.cat(all_labels).squeeze().numpy()  #"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
